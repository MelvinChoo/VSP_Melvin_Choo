{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56fb16cb",
   "metadata": {},
   "source": [
    "## Lab Assignment 6 -- Regression\n",
    "In this lab, you will complete an exercises related to the lecture material on regression. Then, you will compete with your fellow classmates to see who can best predict housing prices. \n",
    "\n",
    "**IMPORTANT:** Before submitting, make sure you restart the kernel and run all cells sequentially. After all cells have executed, then save the file for submission.  This is very important for grading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa22484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this line\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "np.random.seed(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8137271",
   "metadata": {},
   "source": [
    "## Exercise 1 -- Generating & Analyzing Fake Data\n",
    "In this exercise, we will generate some fake data as we did in the lecture on regression trees. Then, we will use it on a series of regression problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509af2e9",
   "metadata": {},
   "source": [
    "## Exercise 1a -- Generating the Data\n",
    "Complete the following steps:\n",
    "1. Define a function called `generate_data` that takes two arguments, an integer `n` and a boolean `square`. `square` should have a default argument of `False`.\n",
    "2. Generate an array called `X` and set it equal to `np.random.randn((n,1))`. This creates an $n$-vector of [**standard normal random variables**](https://en.wikipedia.org/wiki/Normal_distribution).\n",
    "3. Turn `X` into an $nx2$ array by concatenating it with an $n$-vector of ones (**Hint**: use `np.ones((n,1))` and `np.concatenate()`). Make sure that the array of ones serves as the first column. \n",
    "3. Define an array called `beta` and set it equal to the array [1, 3.14]\n",
    "4. Define a variable called `epsilon` and set it equal to `np.random.randn(n)*0.3` \n",
    "5. Then, using `X`, `beta`, and `epsilon`, create a variable named `y` which is equal to \n",
    "    - `np.matmul(X, beta) + epsilon` if square is `False`\n",
    "    - `np.matmul(X ** 2, beta) + epsilon` if `square` is `True`.\n",
    "    \n",
    "6. Your output should return `X`and `y`\n",
    "7. Test your function in the cell below with `n=100` and no argument for `square`. Save the output to `X100` and `y100` respectively. Afterwards, print `y100[50]`.\n",
    "\n",
    "Answer the following questions in the Markdown cell below:\n",
    "1. Is this a bivariate or multivariate linear regression model? Why?\n",
    "2. What is the purpose of  including this `epsilon`? What aspect of real data are we trying to mimic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca021c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, square=False):\n",
    "    # Generate an n-vector of standard normal random variables\n",
    "    X = np.random.randn(n, 1)\n",
    "    \n",
    "    # Turn X into an n x 2 array by concatenating it with an n-vector of ones\n",
    "    X = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "    \n",
    "    # Define the beta array\n",
    "    beta = np.array([1, 3.14])\n",
    "    \n",
    "    # Define epsilon\n",
    "    epsilon = np.random.randn(n) * 0.3\n",
    "    \n",
    "    # Create the variable y based on the value of square\n",
    "    if square:\n",
    "        y = np.matmul(X ** 2, beta) + epsilon\n",
    "    else:\n",
    "        y = np.matmul(X, beta) + epsilon\n",
    "    \n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1a -- Test function and print\n",
    "X100, y100 = generate_data(100)\n",
    "\n",
    "# Print the 50th element of y100\n",
    "print(y100[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d47ea4",
   "metadata": {},
   "source": [
    "### Reponse to Exercise 1a\n",
    "\n",
    "1. This is a bivariate linear regression model. This is because the argument square is set to 'false' which means y = np.matmul(X, beta) + epsilon.\n",
    "2. The purpose of including epsilon is to introduce random noise into the model. In the real-world, measurements and observations often have imperfections that cannot be fully explained by the predictor variables alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ee923",
   "metadata": {},
   "source": [
    "## Exercise 1b -- Standard Linear Regression\n",
    "Using `sklearn`, fit a linear regression model on `y100` and `X100`. When intializing your model, set `fit_intercept` equal to `False` and call your linear model `lr_model_1`. Then, print the estimated coefficients and answer the following question in the Markdown cell below.\n",
    "- What are the coefficient estimates? What values are they close to? Why does this make sense?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9955e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1b -- fit regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming you have already generated X100 and y100\n",
    "# Initialize the linear regression model\n",
    "lr_model_1 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Fit the model to the data\n",
    "lr_model_1.fit(X100, y100)\n",
    "\n",
    "# Print the coefficients\n",
    "print(\"Coefficients:\", lr_model_1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d5d044",
   "metadata": {},
   "source": [
    "### Response to Exercise 1b\n",
    "The coefficient estimates represents the coefficients of Beta 0 and Beta 1 where Beta 0 represents the intercept of the linar model and Beta 1 represents the slope of the linear relationship between the independent variable and the dependent variable. It makes sense because it is almost equal to the first initialisation of the array called beta set to [1, 3.14]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c9960",
   "metadata": {},
   "source": [
    "## Exercise 1c -- Linear Regression with Quadratic Terms\n",
    "Using `generate_data(100, True)`, create two variables `y100_2` and `X100_2`. Then, repeat the steps from **Exercise 1b** above using `X100_2` and `y100_2` instead of `X_100` and `y_100`.  Call your new model `linear_model_2`.\n",
    "\n",
    "Answer the following questions in the Markdown cell below:\n",
    "\n",
    "1. What are the coefficient estimates? Are they similar to the coefficients from **Exercise 1b**? Why or why not?\n",
    "\n",
    "If your estimates were not similar, create a variable `X100_2_sq` in the third cell below that can be used instead of `X100_2` so that your estimates are similar again. Repeat the same process again but call your `lr_model_3`. Print your new estimated coefficients.\n",
    "\n",
    "In the markdown cell below, answer the following question:\n",
    "\n",
    "2. How did you modify `X100_2` to attain similar coefficients? Why did this work? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be504991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1c -- generate variables and repeat regression fit\n",
    "X100_2, y100_2 = generate_data(100, True)\n",
    "\n",
    "\n",
    "#print(X100_2)\n",
    "#print(y100_2)\n",
    "\n",
    "lr_model_2 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Fit the model to the data\n",
    "lr_model_2.fit(X100_2, y100_2)\n",
    "\n",
    "# Print the coefficients\n",
    "print(\"Coefficients:\", lr_model_2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064117e4",
   "metadata": {},
   "source": [
    "### Response to Exercise 1c -- Question 1\n",
    "No, they are different from the coefficients in Exercise 1b. They are different due to the transformation of the independent varibale x (using x squared instead of x). This demonstrates how the transformation of the independent variable can significantly impact the resulting coefficient estimates in a regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1c -- modify X100_2 and run new regression\n",
    "\n",
    "# Apply squared transformation\n",
    "X100_2_sq = X100_2 ** 2\n",
    "\n",
    "#print(X100_2_sq)\n",
    "\n",
    "# Initialize the linear regression model\n",
    "lr_model_3 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Fit the model to the transformed data\n",
    "lr_model_3.fit(X100_2_sq, y100_2)\n",
    "\n",
    "# Print the new coefficients\n",
    "print(\"New Coefficients:\", lr_model_3.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67e55d",
   "metadata": {},
   "source": [
    "### Response to Exercise 1c -- Quesiton 2\n",
    "To attain similar Beta coefficients, square the second column of X100_2. This will make the second column identical as when we run the function, X100_2, y100_2 = generate_data(100, True). Hence, by only changing the variable and not running the function, we will receive a similar Beta result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebaa1d3",
   "metadata": {},
   "source": [
    "### Exercise 1d -- Unnecessary Quadratic Terms\n",
    "Now we are going to see what happens when we estimate a model that only has linear terms using both linear and quadratic terms. Complete the following steps:\n",
    "1. Create an $nx3$ array called `X100_ext` by concatenating `X100` with a column that is equal to the square of elements in the second column. Make sure this new column is the third column. Note that `np.concatenate` requires that both arrays are of the same dimension. You may have to use the method [`.reshape()`](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html).\n",
    "2. Now repeat the steps of **Exercise 1b** with `X100_ext`. Make sure you print the estimated coefficients.\n",
    "\n",
    "Answer the following questions in the Markdown cell below:\n",
    "1. Are the first two coefficients different from their respective counterparts in part **Exercise 1b**? Why do you think this is?\n",
    "2.  Is the third coeffcient close to 0 or large? Why do you think this is?\n",
    "3. Do you think these estimates are accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba894f15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 1d -- Create X100_ext here\n",
    "# Create X100_ext with a new column for the squared term\n",
    "X100_ext = np.concatenate([X100, (X100[:, 1] ** 2).reshape(-1, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50115717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1d -- Repeat exercise 1b here\n",
    "# Initialize the linear regression model\n",
    "lr_model_4 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Fit the model to the extended data\n",
    "lr_model_4.fit(X100_ext, y100)\n",
    "\n",
    "# Print the estimated coefficients\n",
    "print(\"Estimated Coefficients with Extended X100:\")\n",
    "print(lr_model_4.coef_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18eab25",
   "metadata": {},
   "source": [
    "### Response to Exercise 1d\n",
    "1. No, they are not different from their respective counterparts in part Exercise 1b. This is the first and second terms correspond to the intercept term (constant term) in the model as well as the linear term in the model respectively. \n",
    "2. The third coefiicient is close to zero as the third column is equal to the square of elements in the second column. This means that it does not significantly improve the modelâ€™s ability to explain the variance in the response variable, y. \n",
    "3. Yes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3fd11d",
   "metadata": {},
   "source": [
    "## Exercise 1e -- Regression Plots\n",
    "Following the notes in the plotting lectures complete the following steps:\n",
    "1. Using `subplots()` initialize a figure with 4 figures in a $2x2$ grid\n",
    "2. Plot the following in the indicated location. \n",
    "    - **Top-left**  -- a line plot of `lr_model_1` and a scatter plot of the data used to generate `lr_model_1`.\n",
    "    - **Bottom-left**  -- a line plot of `lr_model_2` and a scatter plot of the data used to generate `lr_model_2`\n",
    "    - **Bottom-right** -- a line plot of `lr_model_3` and a scatter plot of the data used to generate `lr_model_3`\n",
    "    - **Top-right** -- a line plot of `lr_model_4` and a scatter plot of the data used to generate `lr_model_4`\n",
    "    \n",
    "For the plots above, \n",
    "- make your lines red, \n",
    "- title your plots (e.g. \"Linear Model 1\"),\n",
    "- use `np.linspace(-4,4,200)` as your domain when plotting the lines,\n",
    "- call `fig.tight_layout()` so your plot is not cluttered\n",
    "\n",
    "3. Using the `metrics` submodule of `sklearn`, print the `in-sample` mean squared errors of each model using f strings. Your stings should looke like this: \"MSE of Linear Model 1 is .3\"  \n",
    "**Hints:** . \n",
    "- To plot on the top left axis, you will need to work with `axes.flat[0]` . The remaining axes are indexed by 1, 2, and 3.\n",
    "- If you choose to used the `.predict()` to plot your lines, keep in mind you need to provide it with the correctly shaped input. \n",
    "- When calculating the means within a loop, it may hep to create a list that contains the four linear models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb2cad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Exercise 1e -- plots\n",
    "\n",
    "# Create the 2x2 grid of subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Define the domain for the line plots\n",
    "domain = np.linspace(-4, 4, 200).reshape(-1, 1)\n",
    "\n",
    "# Titles for the plots\n",
    "titles = [\"Linear Model 1\", \"Quadratic Model 2\", \"Transformed Model 3\", \"Extended Model 4\"]\n",
    "\n",
    "# Scatter plot and line plot for each model\n",
    "# Top-left: Linear Model 1\n",
    "axes.flat[0].scatter(X100[:, 1], y100, color='blue', alpha=0.5, label='Data')\n",
    "axes.flat[0].plot(domain, lr_model_1.predict(np.concatenate([np.ones_like(domain), domain], axis=1)), color='red', label='Model')\n",
    "axes.flat[0].set_title(\"Linear Model 1\")\n",
    "axes.flat[0].legend()\n",
    "\n",
    "# Bottom-left: Quadratic Model 2\n",
    "axes.flat[2].scatter(X100_2[:, 1], y100_2, color='blue', alpha=0.5, label='Data')\n",
    "axes.flat[2].plot(domain, lr_model_2.predict(np.concatenate([np.ones_like(domain), domain], axis=1)), color='red', label='Model')\n",
    "axes.flat[2].set_title(\"Quadratic Model 2\")\n",
    "axes.flat[2].legend()\n",
    "\n",
    "# Bottom-right: Transformed Model 3\n",
    "axes.flat[3].scatter(X100_2_sq[:, 1], y100_2, color='blue', alpha=0.5, label='Data')\n",
    "axes.flat[3].plot(domain, lr_model_3.predict(np.concatenate([np.ones_like(domain), domain], axis=1)), color='red', label='Model')\n",
    "axes.flat[3].set_title(\"Transformed Model 3\")\n",
    "axes.flat[3].legend()\n",
    "\n",
    "# Top-right: Extended Model 4\n",
    "axes.flat[1].scatter(X100[:, 1], y100, color='blue', alpha=0.5, label='Data')\n",
    "axes.flat[1].plot(domain, lr_model_4.predict(np.concatenate([np.ones_like(domain), domain, domain**2], axis=1)), color='red', label='Model')\n",
    "axes.flat[1].set_title(\"Extended Model 4\")\n",
    "axes.flat[1].legend()\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de691e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 1e -- mean squared errors\n",
    "from sklearn import metrics\n",
    "# Calculate and print the mean squared errors\n",
    "models = [lr_model_1, lr_model_2, lr_model_3, lr_model_4]\n",
    "data = [(X100, y100), (X100_2, y100_2), (X100_2_sq, y100_2), (X100_ext, y100)]\n",
    "\n",
    "for i, (model, (X, y)) in enumerate(zip(models, data)):\n",
    "    y_pred = model.predict(X)\n",
    "    mse = metrics.mean_squared_error(y, y_pred)\n",
    "    print(f\"MSE of {titles[i]} is {mse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473fece",
   "metadata": {},
   "source": [
    "For the results above, Qudratic Model 2 has x square in the equation which does not equate to Transformed Model 3 which is still linear and does not have x square in the equation. Hence, do not expect the MSE to be the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81baa272",
   "metadata": {},
   "source": [
    "### Exercise 1f -- Functional Misspecification\n",
    "**Functional Misspecification** is used to describe the situation where the functional form of the regression model we are estimating is not the same as the functional form of the true data generating process. Answer the following question in the markdown cell below:\n",
    "- Which of the four linear models do you think are well-specified? Which ones are not? Is including extra terms problematic when it comes to being well-specified. What about excluding the terms found in the true data generating process?\n",
    "- How does misspecification manifest itself in the plots? How about in the mean squared errors? \n",
    "- After doing this exercise, do you think it is important to investigate the relationship between variables before determining your regression specification? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3f378",
   "metadata": {},
   "source": [
    "### Response to Exercise 1f\n",
    "\n",
    "1. The Well-Specified Models are Linear Model 1, Transformed Model 3 & Extended Model 4. The non Well-Specified Model is Quadratic Model 2. Including extra terms (like in Extended Model 4) is generally not problematic as it allows the model to capture more complex relationships. Excluding terms such as by fitting lr_model_2 directly with X100_2 without squaring the X values again, can lead to significant misspecification, as the model will fail to capture the true underlying relationship. \n",
    "\n",
    "2. The Well-specified models show good alignment between the predicted line and the scatter plot of the data points. Misspecified models show a clear mismatch between the predicted line and the scatter plot, indicating poor model performance. As for MSEs, well-specified models have low MSE, reflecting accurate predictions and good model fit. Misspecified models have high MSE, indicating poor predictions and significant deviations from the true data generating process.\n",
    "\n",
    "3. Investigating the relationship between variables before determining regression specification is crucial. Understanding the true data generating process allows you to specify the correct functional form for your model, leading to better fit and more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7d5c8",
   "metadata": {},
   "source": [
    "### Exercise 1g -- Lasso\n",
    "Finally, we will run lasso on our fake data. Complete the following steps:\n",
    "1. Generate `X1000` and `y1000` using `generate_data(1000)`\n",
    "2. Create an `1000x3` array called `X1000_ext` which is created in a anaglous fashion to `X100_ext`. \n",
    "3. Follow the lecture notes to create a standardized version of `X1000_ext` called `X1000_ext_scl`. You will need to import the `preprocessing` submodule of sklearn.\n",
    "4. Check to make sure your means and variances. You should see that everything looks good except for our intercept has a variance of $0$. You actually do not want to standardize an intercept but we still need it! Replace the first column of  `X1000_ext_scl` with a fresh column of ones using `np.ones(1000)`.\n",
    "5. Create a dataframe version of `X1000_ext_scl` called `X_lasso_df` and rename the columns to \"intercept\", \"x\", and \"x_sq\" respectively.Then call `X_lasso_df` at the bottom of the cell.\n",
    "6. Copy and paste the Lasso path code from the lecture notes into the second cell below. Adapt it so it works for `X_lasso_df` and `y1000`.\n",
    "\n",
    "\n",
    "In the Markdown cell below, answer the following questions:\n",
    "1. Characterize `X_sq`'s lasso path. Why was this behavior predictable?    Reference linear model 4 or the true DGP in your answer.\n",
    "2. Without checking, do you think a low or high value for alpha would be chosen by cross validation? To help you answer this question, think about what the true coefficients are and whether or not higher alphas bring the lasso coefficients closer to their true counterparts or farther away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250202b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Exercise 1g -- Steps 1-5\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X1000, y1000 = generate_data(1000)\n",
    "\n",
    "#print(X1000)\n",
    "\n",
    "# Create X1000_ext with a new column for the squared term\n",
    "X1000_ext = np.concatenate([X1000, (X1000[:, 1] ** 2).reshape(-1, 1)], axis=1)\n",
    "\n",
    "#print(X1000_ext)\n",
    "\n",
    "# Standardize X1000_ext\n",
    "scaler = StandardScaler()\n",
    "X1000_ext_scl = scaler.fit_transform(X1000_ext)\n",
    "\n",
    "#print(X1000_ext_scl)\n",
    "\n",
    "# Check means and variances\n",
    "print(\"Means:\", X1000_ext_scl.mean(axis=0))\n",
    "print(\"Variances:\", X1000_ext_scl.var(axis=0))\n",
    "\n",
    "# Replace the first column with a column of ones\n",
    "X1000_ext_scl[:, 0] = np.ones(1000)\n",
    "\n",
    "# Create a dataframe\n",
    "X_lasso_df = pd.DataFrame(X1000_ext_scl, columns=[\"intercept\", \"x\", \"x_sq\"])\n",
    "\n",
    "# Display the dataframe\n",
    "X_lasso_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfec5de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Exercise 1g -- Step 5 copy code here\n",
    "from sklearn.linear_model import lasso_path\n",
    "from itertools import cycle\n",
    "\n",
    "# Define the alphas range for Lasso path\n",
    "alphas = np.exp(np.linspace(-5, 1, 100))\n",
    "\n",
    "# Calculate Lasso path\n",
    "alphas, coefs_lasso, _ = lasso_path(X_lasso_df, y1000, alphas=alphas, max_iter=1000000)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "colors = ['#165aa7', '#cb495c', '#fec630']\n",
    "color_cycle = cycle(colors)\n",
    "log_alphas = np.log10(alphas)\n",
    "\n",
    "for coef_l, c, name in zip(coefs_lasso, color_cycle, X_lasso_df.columns):\n",
    "    ax.plot(log_alphas, coef_l, c=c, label=name)\n",
    "\n",
    "ax.set_xlabel('Log10(alpha)')\n",
    "ax.set_ylabel('Standardized Lasso coefficients')\n",
    "ax.set_title('Lasso Path')\n",
    "ax.legend()\n",
    "ax.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e6b209",
   "metadata": {},
   "source": [
    "1. This behaviour is predictable because the true DGP is y = 1 + 3.14 * x + e, without a squared term for x. This implies that the squared term x_sq is not significant. Hence, this corresponds to the Lasso path for x_sq which shows that its coefficient reamins at zero as the value of alpha increases.\n",
    "\n",
    "2. A low value will be chosen for cross validation. This is ideal as lower alpha values allow the model to retain the significant coefficients closer to their true values. On the other hand, too high an alpha value will increase the penalty. This causes significant coefficients to be reduced unnecessarily and move significant ones away from their true values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d059dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
